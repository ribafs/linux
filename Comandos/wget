The Ultimate Wget Download Guide With 15 Awesome Examples
by SathiyaMoorthy on September 28, 2009

15 Practical Examples to Download Images and Videos from Internetwget utility is the best option to download files from internet. wget can pretty much handle all complex download situations including large file downloads, recursive downloads, non-interactive downloads, multiple file downloads etc.,

In this article let us review how to use wget for various download scenarios using 15 awesome wget examples.
1. Download Single File with wget

The following example downloads a single file from internet and stores in the current directory.

$ wget http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2

While downloading it will show a progress bar with the following information:

    %age of download completion (for e.g. 31% as shown below)
    Total amount of bytes downloaded so far (for e.g. 1,213,592 bytes as shown below)
    Current download speed (for e.g. 68.2K/s as shown below)
    Remaining time to download (for e.g. eta 34 seconds as shown below)

Download in progress:

$ wget http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2
Saving to: `strx25-0.9.2.1.tar.bz2.1'

31% [=================> 1,213,592   68.2K/s  eta 34s

Download completed:

$ wget http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2
Saving to: `strx25-0.9.2.1.tar.bz2'

100%[======================>] 3,852,374   76.8K/s   in 55s    

2009-09-25 11:15:30 (68.7 KB/s) - `strx25-0.9.2.1.tar.bz2' saved [3852374/3852374]

2. Download and Store With a Different File name Using wget -O

By default wget will pick the filename from the last word after last forward slash, which may not be appropriate always.

Wrong: Following example will download and store the file with name: download_script.php?src_id=7701

$ wget http://www.vim.org/scripts/download_script.php?src_id=7701

Even though the downloaded file is in zip format, it will get stored in the file as shown below.

$ ls
download_script.php?src_id=7701

Correct: To correct this issue, we can specify the output file name using the -O option as:

$ wget -O taglist.zip http://www.vim.org/scripts/download_script.php?src_id=7701

3. Specify Download Speed / Download Rate Using wget –limit-rate

While executing the wget, by default it will try to occupy full possible bandwidth. This might not be acceptable when you are downloading huge files on production servers. So, to avoid that we can limit the download speed using the –limit-rate as shown below.

In the following example, the download speed is limited to 200k

$ wget --limit-rate=200k http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2

4. Continue the Incomplete Download Using wget -c

Restart a download which got stopped in the middle using wget -c option as shown below.

$ wget -c http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2

This is very helpful when you have initiated a very big file download which got interrupted in the middle. Instead of starting the whole download again, you can start the download from where it got interrupted using option -c

Note: If a download is stopped in middle, when you restart the download again without the option -c, wget will append .1 to the filename automatically as a file with the previous name already exist. If a file with .1 already exist, it will download the file with .2 at the end.
5. Download in the Background Using wget -b

For a huge download, put the download in background using wget option -b as shown below.

$ wget -b http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2
Continuing in background, pid 1984.
Output will be written to `wget-log'.

It will initiate the download and gives back the shell prompt to you. You can always check the status of the download using tail -f as shown below.

$ tail -f wget-log
Saving to: `strx25-0.9.2.1.tar.bz2.4'

     0K .......... .......... .......... .......... ..........  1% 65.5K 57s
    50K .......... .......... .......... .......... ..........  2% 85.9K 49s
   100K .......... .......... .......... .......... ..........  3% 83.3K 47s
   150K .......... .......... .......... .......... ..........  5% 86.6K 45s
   200K .......... .......... .......... .......... ..........  6% 33.9K 56s
   250K .......... .......... .......... .......... ..........  7%  182M 46s
   300K .......... .......... .......... .......... ..........  9% 57.9K 47s

Also, make sure to review our previous multitail article on how to use tail command effectively to view multiple files.
6. Mask User Agent and Display wget like Browser Using wget –user-agent

Some websites can disallow you to download its page by identifying that the user agent is not a browser. So you can mask the user agent by using –user-agent options and show wget like a browser as shown below.

$ wget --user-agent="Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.3) Gecko/2008092416 Firefox/3.0.3" URL-TO-DOWNLOAD

7. Test Download URL Using wget –spider

When you are going to do scheduled download, you should check whether download will happen fine or not at scheduled time. To do so, copy the line exactly from the schedule, and then add –spider option to check.

$ wget --spider DOWNLOAD-URL

If the URL given is correct, it will say

$ wget --spider download-url
Spider mode enabled. Check if remote file exists.
HTTP request sent, awaiting response... 200 OK
Length: unspecified [text/html]
Remote file exists and could contain further links,
but recursion is disabled -- not retrieving.

This ensures that the downloading will get success at the scheduled time. But when you had give a wrong URL, you will get the following error.

$ wget --spider download-url
Spider mode enabled. Check if remote file exists.
HTTP request sent, awaiting response... 404 Not Found
Remote file does not exist -- broken link!!!

You can use the spider option under following scenarios:

    Check before scheduling a download.
    Monitoring whether a website is available or not at certain intervals.
    Check a list of pages from your bookmark, and find out which pages are still exists.

8. Increase Total Number of Retry Attempts Using wget –tries

If the internet connection has problem, and if the download file is large there is a chance of failures in the download. By default wget retries 20 times to make the download successful.

If needed, you can increase retry attempts using –tries option as shown below.

$ wget --tries=75 DOWNLOAD-URL

9. Download Multiple Files / URLs Using Wget -i

First, store all the download files or URLs in a text file as:

$ cat > download-file-list.txt
URL1
URL2
URL3
URL4

Next, give the download-file-list.txt as argument to wget using -i option as shown below.

$ wget -i download-file-list.txt

10. Download a Full Website Using wget –mirror

Following is the command line which you want to execute when you want to download a full website and made available for local viewing.

$ wget --mirror -p --convert-links -P ./LOCAL-DIR WEBSITE-URL

    –mirror : turn on options suitable for mirroring.
    -p : download all files that are necessary to properly display a given HTML page.
    –convert-links : after the download, convert the links in document for local viewing.
    -P ./LOCAL-DIR : save all the files and directories to the specified directory.

11. Reject Certain File Types while Downloading Using wget –reject

You have found a website which is useful, but don’t want to download the images you can specify the following.

$ wget --reject=gif WEBSITE-TO-BE-DOWNLOADED

12. Log messages to a log file instead of stderr Using wget -o

When you wanted the log to be redirected to a log file instead of the terminal.

$ wget -o download.log DOWNLOAD-URL

13. Quit Downloading When it Exceeds Certain Size Using wget -Q

When you want to stop download when it crosses 5 MB you can use the following wget command line.

$ wget -Q5m -i FILE-WHICH-HAS-URLS

Note: This quota will not get effect when you do a download a single URL. That is irrespective of the quota size everything will get downloaded when you specify a single file. This quota is applicable only for recursive downloads.
14. Download Only Certain File Types Using wget -r -A

You can use this under following situations:

    Download all images from a website
    Download all videos from a website
    Download all PDF files from a website

$ wget -r -A.pdf http://url-to-webpage-with-pdfs/

15. FTP Download With wget

You can use wget to perform FTP download as shown below.

Anonymous FTP download using Wget

$ wget ftp-url

FTP download using wget with username and password authentication.

$ wget --ftp-user=USERNAME --ftp-password=PASSWORD DOWNLOAD-URL

If you liked this article, please bookmark it with delicious or Stumble.
If you enjoyed this article, you might also like..

-------------
Linux and Unix wget command tutorial with examples
Tutorial on using wget, a Linux and UNIX command for downloading files from the Internet. Examples of downloading a single file, downloading multiple files, resuming downloads, throttling download speeds and mirroring a remote site.

Estimated reading time: 7 minutes
Table of contents

        What is the wget command?
        How to download a file
        How to change the name of a downloaded file
        How to turn off output
        How to turn off verbose output
        How to resume a download
        How to download multiple URLs
        How to throttle download speeds
        How to download in the background
        How to log output to a file
        How to mirror a website
        How to find broken links
        How to set the user agent
        How to view response headers
        Further reading

Terminal showing wget man page
What is the wget command?

The wget command is a command line utility for downloading files from the Internet. It supports downloading multiple files, downloading in the background, resuming downloads, limiting the bandwidth used for downloads and viewing headers. It can also be used for taking a mirror of a site and can be combined with other UNIX tools to find out things like broken links on a site.
How to download a file

To download a file with wget pass the resource your would like to download.

wget http://archlinux.mirrors.uk2.net/iso/2016.09.03/archlinux-2016.09.03-dual.iso
--2016-09-16 11:04:40--  http://archlinux.mirrors.uk2.net/iso/2016.09.03/archlinux-2016.09.03-dual.iso
Resolving archlinux.mirrors.uk2.net (archlinux.mirrors.uk2.net)... 83.170.94.3
Connecting to archlinux.mirrors.uk2.net (archlinux.mirrors.uk2.net)|83.170.94.3|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 792723456 (756M) [application/x-iso9660-image]
Saving to: ‘archlinux-2016.09.03-dual.iso’

archlinux-2016.09.03-dual.iso   2%[>                                                ]  18.48M  2.39MB/s    eta 5m 23s

The output of the command shows wget connecting to the remote server and the HTTP response. In this case we can see that the file is 758M and is a MIME type of application/x-iso9660-image. The file will be saved as archlinux-2016.09.03-dual.iso. Finally the standard output of wget provides a progress bar. This contains the following from left to right.

    The name of the file
    A thermometer style progress bar with the percentage downloaded
    The amount of the file that has been downloaded
    The download speed
    The estimated time to complete the download

How to change the name of a downloaded file

To change the name of the file that is saved locally pass the -O option. This can be useful if saving a web page with query parameters. In the following example suppose that the URL https://petition.parliament.uk/petitions?page=2&state=all is to be downloaded.

wget "https://petition.parliament.uk/petitions?page=2&state=all"
--2016-09-16 11:15:26--  https://petition.parliament.uk/petitions?page=2&state=all
Resolving petition.parliament.uk (petition.parliament.uk)... 52.48.191.89, 54.72.19.76
Connecting to petition.parliament.uk (petition.parliament.uk)|52.48.191.89|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 25874 (25K) [text/html]
Saving to: ‘petitions?page=2&state=all’

petitions?page=2&state=all    100%[================================================>]  25.27K  --.-KB/s    in 0.02s

2016-09-16 11:15:27 (1.55 MB/s) - ‘petitions?page=2&state=all’ saved [25874/25874]

Using the ls command shows that the file has been saved as petitions?page=2&state=all. To specify a different filename the -O option may be used.

wget "https://petition.parliament.uk/petitions?page=2&state=all" -O petitions.html
--2016-09-16 11:18:04--  https://petition.parliament.uk/petitions?page=2&state=all
Resolving petition.parliament.uk (petition.parliament.uk)... 52.48.191.89, 54.72.19.76
Connecting to petition.parliament.uk (petition.parliament.uk)|52.48.191.89|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 25874 (25K) [text/html]
Saving to: ‘petitions.html’

petitions.html                100%[================================================>]  25.27K  --.-KB/s    in 0.02s

2016-09-16 11:18:06 (1.38 MB/s) - ‘petitions.html’ saved [25874/25874]

How to turn off output

To turn off output use the -q option. This prevents wget from writing to standard output and makes it totally silent.

wget -q http://www.bbc.co.uk
ls
index.html

How to turn off verbose output

To turn off verbose output use the -nv option. This limits the output of wget but provides some useful information.

wget -nv http://www.bbc.co.uk
2016-09-16 11:23:31 URL:http://www.bbc.co.uk/ [172348/172348] -> "index.html" [1]

How to resume a download

To resume a download use the -c option. This makes wget for a file in the folder that the command was run from of the same name as the remote file. If there is a file then wget will start the download from the end of the local file. This can be useful if a remote server dropped a connection in the middle of a download or if your network dropped during a download. In the following example the download of the Arch Linux iso is resumed.

wget -c http://archlinux.mirrors.uk2.net/iso/2016.09.03/archlinux-2016.09.03-dual.iso
--2016-09-16 11:30:15--  http://archlinux.mirrors.uk2.net/iso/2016.09.03/archlinux-2016.09.03-dual.iso
Resolving archlinux.mirrors.uk2.net (archlinux.mirrors.uk2.net)... 83.170.94.3
Connecting to archlinux.mirrors.uk2.net (archlinux.mirrors.uk2.net)|83.170.94.3|:80... connected.
HTTP request sent, awaiting response... 206 Partial Content
Length: 792723456 (756M), 734083359 (700M) remaining [application/x-iso9660-image]
Saving to: ‘archlinux-2016.09.03-dual.iso’

archlinux-2016.09.03-dual.iso   7%[+++                                              ]  58.36M  2.02MB/s 

The download is resumed from the end of the local file.
How to download multiple URLs

To download multiple files at once pass the -i option and a file with a list of the URLs to be downloaded. URLs should be on a separate line. In the following example a listed of Linux ISOs is saved in a file called isos.txt.

http://archlinux.mirrors.uk2.net/iso/2016.09.03/archlinux-2016.09.03-dual.iso
https://www.mirrorservice.org/sites/cdimage.ubuntu.com/cdimage/releases/16.04/release/ubuntu-16.04.1-server-arm64.iso
http://mirror.ox.ac.uk/sites/mirror.centos.org/6.8/isos/x86_64/CentOS-6.8-x86_64-minimal.iso

To download these files in sequence pass the name of the file to the -i option.

wget -i isos.txt

How to throttle download speeds

To throttle download speeds use the --limit-rate option. This will limit the amount of bandwidth available to wget and can be useful to prevent wget consuming all the available bandwidth. In the following example the download is limited to 400k.

wget --limit-rate 400k http://archlinux.mirrors.uk2.net/iso/2016.09.03/archlinux-2016.09.03-dual.iso
--2016-09-16 12:05:12--  http://archlinux.mirrors.uk2.net/iso/2016.09.03/archlinux-2016.09.03-dual.iso
Resolving archlinux.mirrors.uk2.net (archlinux.mirrors.uk2.net)... 83.170.94.3
Connecting to archlinux.mirrors.uk2.net (archlinux.mirrors.uk2.net)|83.170.94.3|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 792723456 (756M) [application/x-iso9660-image]
Saving to: ‘archlinux-2016.09.03-dual.iso.6’

archlinux-2016.09.03-dual.iso   0%[                                                 ]   1.42M   400KB/s    eta 32m 12s

It is possible to see in the output the download is limited to 400KB/s.
How to download in the background

To download in the background use the background use the -b option. This can be useful in that no terminal is needed to run the download. The wget command will return the pid number of the process and also the name of the file that output will be logged to.

wget -b http://archlinux.mirrors.uk2.net/iso/2016.09.03/archlinux-2016.09.03-dual.iso
Continuing in background, pid 4135.
Output will be written to ‘wget-log’.

How to log output to a file

To direct wget output to a log file use the -o option and pass the name of a file.

wget -o iso.log http://archlinux.mirrors.uk2.net/iso/2016.09.03/archlinux-2016.09.03-dual.iso

To append output to a file use the -a option. If no file is present it will be created.

wget -a iso.log http://archlinux.mirrors.uk2.net/iso/2016.09.03/archlinux-2016.09.03-dual.iso

How to mirror a website

The wget command can mirror a remote website for local, offline browsing. It has many options for converting links and limiting downloads of certain file types.

wget -mk -w 20 http://www.example.com/

The options here are as follows

    -m turn on mirroring
    -k make links suitable for local browsing
    -w wait 20 seconds between each download. This is to avoid placing extra strain on the remote server.

Once the command has finished a local copy of the remote site will be available.
How to find broken links

To find broken links on a site wget can spider the site and present a log file that can be used to identify broken links.

wget -o wget.log -r -l 10 --spider http://example.com

The options in this command are as follows

    -o send the output to a file for use later.
    -r download recursively (so follow links).
    -l specify the maximum level of recursion.
    --spider set wget to spider mode.

The output file can then generate a list of unique broken links with the following command

grep -B 2 '404' wget.log | grep "http" | cut -d " " -f 4 | sort -u

How to set the user agent

To set the user agent pass the -U option. This allows an arbitrary string to be set for the user agent. This can be useful if the site you are trying to download blocks access to the wget user agent.

wget -U 'My-User-Agent' http://www.foo.com

A list of User Agents is available here.
How to view response headers

To view response headers pass the -S option. This will show the response headers from the server as well as downloading the file.

wget -S http://www.bbc.co.uk
--2016-09-16 14:20:52--  http://www.bbc.co.uk/
Resolving www.bbc.co.uk (www.bbc.co.uk)... 212.58.244.70, 212.58.246.94
Connecting to www.bbc.co.uk (www.bbc.co.uk)|212.58.244.70|:80... connected.
HTTP request sent, awaiting response...
  HTTP/1.1 200 OK
  Server: nginx
  Content-Type: text/html; charset=utf-8
  ...
Length: 171925 (168K) [text/html]
Saving to: ‘index.html’


index.html                    100%[================================================>] 167.90K  --.-KB/s    in 0.1s

2016-09-16 14:20:52 (1.44 MB/s) - ‘index.html’ saved [171925/171925]

To just view the headers and not download the file use the --spider option.

wget -S --spider http://www.bbc.co.uk
Spider mode enabled. Check if remote file exists.
--2016-09-16 14:23:42--  http://www.bbc.co.uk/
Resolving www.bbc.co.uk (www.bbc.co.uk)... 212.58.244.67, 212.58.246.91
Connecting to www.bbc.co.uk (www.bbc.co.uk)|212.58.244.67|:80... connected.
HTTP request sent, awaiting response...
  HTTP/1.1 200 OK
  Server: nginx
  Content-Type: text/html; charset=utf-8
  ...
Length: 171933 (168K) [text/html]
Remote file exists and could contain further links,
but recursion is disabled -- not retrieving.

Further reading

    wget man page
    All the Wget Commands You Should Know
    Wget examples
    Mastering Wget
    wget Wikipedia page

Have an update or suggestion for this article? You can edit it here and send me a pull request.
Tags

    UNIX
    Linux

Recent Posts

    Using HashiCorp Vault with LDAP
    How to use HashiCorp Vault to setup an LDAP backed secret store with read-only access for users in groups and read-write access for specific users

    Linux and Unix xargs command tutorial with examples
    Tutorial on using xargs, a UNIX and Linux command for building and executing command lines from standard input. Examples of cutting by character, byte position, cutting based on delimiter and how to modify the output delimiter.

    Copy a file in Go
    How to copy a file in Go. The ioutil package does not offer a shorthand way of copying a file. Instead the os package should be used.

https://shapeshed.com/unix-wget/
